<!DOCTYPE html>
<html lang="en"><head>
<script src="Chapter7Slides_files/libs/clipboard/clipboard.min.js"></script>
<script src="Chapter7Slides_files/libs/quarto-html/tabby.min.js"></script>
<script src="Chapter7Slides_files/libs/quarto-html/popper.min.js"></script>
<script src="Chapter7Slides_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="Chapter7Slides_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Chapter7Slides_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="Chapter7Slides_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.2.475">

  <meta name="author" content="Holly Steeves">
  <title>Chapter 7</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="Chapter7Slides_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="Chapter7Slides_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="Chapter7Slides_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="Chapter7Slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="Chapter7Slides_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="Chapter7Slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="Chapter7Slides_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Chapter 7</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
Holly Steeves 
</div>
</div>
</div>

</section>
<section>
<section id="general-concepts-and-criteria" class="title-slide slide level1 center">
<h1>7.1 General Concepts and Criteria</h1>

</section>
<section id="objectives" class="slide level2">
<h2>Objectives</h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li class="fragment"><p>Define and distinguish between a point estimate and a point estimator.</p></li>
<li class="fragment"><p>Define and calculate the mean squared error.</p></li>
<li class="fragment"><p>Define and unbiased estimator and be able to determine if an estimator is unbiased.</p></li>
<li class="fragment"><p>Define and calculate bias.</p></li>
<li class="fragment"><p>Define a minimum variance unbiased estimator and be able to determine if an estimator is the MVUE.</p></li>
<li class="fragment"><p>Define and calculate the standard error of an estimator.</p></li>
</ul>
</section>
<section id="the-branches" class="slide level2 smaller">
<h2>The Branches</h2>
<p>There are 3 branches of probability and statistics:</p>
<div class="columns">
<div class="column" style="width:40%;">
<ul>
<li class="fragment">Descriptive Statistics: Methods to summarize and describe features of the data.</li>
<li class="fragment">Probability: Methods for using known properties of a population to draw conclusions about a sample.</li>
<li class="fragment">Inferential Statistics: Methods for using a sample to draw conclusions about a population.</li>
</ul>
</div><div class="column" style="width:60%;">
<p><img data-src="/Users/hollysteeves/Library/CloudStorage/OneDrive-TheUniversityofWesternOntario/SS2858/Branches.png" alt="Graphic showing population and sample, with an arrow from sample to population labeled as inferential statistics, an arrow from population to sample labeled as probability, and arrows from population and sample to themselves labeled as descriptive statistics."></p>
</div>
</div>
</section>
<section id="inferential-statistics" class="slide level2">
<h2>Inferential Statistics</h2>
<ul>
<li class="fragment"><p>If the population is small, we can calculate <span class="math inline">\(\mu\)</span> by collecting by measuring all individuals in the population.</p></li>
<li class="fragment"><p>Generally speaking, populations are large and this is not feasible, so we collect samples to get <strong>estimates</strong> of the <strong>parameters</strong> instead.</p></li>
</ul>
</section>
<section id="recall-from-chapter-6" class="slide level2">
<h2>Recall from Chapter 6</h2>
<ul>
<li class="fragment"><p>Prior to obtaining data, there is uncertainty in what value the resulting statistic will be.</p></li>
<li class="fragment"><p>Note: Upper case letter: a statistic is also a random variable.</p></li>
<li class="fragment"><p>Lower case letter: The calculated or observed value of the statistic</p></li>
<li class="fragment"><p>A <strong>parameter</strong>, generally denoted by <span class="math inline">\(\theta\)</span>, is a descriptive measure of an unknown probability distribution.</p></li>
<li class="fragment"><p>A <strong>statistic</strong> is a descriptive measure of a sample.</p></li>
</ul>
</section>
<section id="point-estimates-and-estimators" class="slide level2">
<h2>Point Estimates and Estimators</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>A <strong>point estimate</strong> of a parameter <span class="math inline">\(\theta\)</span> is a single number that can be regarded as a sensible value for <span class="math inline">\(\theta\)</span>. A point estimate is obtained by selected a suitable statistic and computing its value from the given sample data. The selected statistic is called the <strong>point estimator</strong> of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="point-estimates-and-estimators-1" class="slide level2 smaller">
<h2>Point Estimates and Estimators</h2>
<p>Example: Consider 20 observations on a dielectric breakdown voltage for pieces of epoxy resin. The line on the normal probability plot appears quite straight, so now we assume that the data is normal with mean <span class="math inline">\(\mu\)</span>. Because normal distributions are symmetric, <span class="math inline">\(\mu\)</span> is also the median. The given observations are then assumed to be the result of a random sample from this normal distribution. Consider the following estimators and estimates for <span class="math inline">\(\mu\)</span>.</p>
<ol type="a">
<li class="fragment">Estimator = <span class="math inline">\(\bar{X}\)</span>, estimate = <span class="math inline">\(\bar{x}\)</span> = 27.793</li>
<li class="fragment">Estimator = <span class="math inline">\(\tilde{X}\)</span>, estimate = <span class="math inline">\(\tilde{x}\)</span> = 27.960</li>
<li class="fragment">Estimator = <span class="math inline">\(\bar{X}_{e} = \frac{(max(X_{i}) - min(X_{i}))}{2}\)</span> = the midrange, estimate = <span class="math inline">\(\bar{x}_{e}\)</span> = 27.670</li>
<li class="fragment">Estimator = <span class="math inline">\(\bar{X}_{tr(10)}\)</span>, estimate = <span class="math inline">\(\bar{x}_{tr(10)}\)</span> = 27.838</li>
</ol>
</section>
<section id="point-estimates-and-estimators-2" class="slide level2">
<h2>Point Estimates and Estimators</h2>
<ul>
<li class="fragment">Since the data comes from a symmetric distribution, each of these is an estimate of the center of the distribution.</li>
<li class="fragment">Which of these estimates is closest to the true value? - We cannot answer this without knowing the true value.</li>
<li class="fragment">A question that can be answered is “Which estimator, when used on other samples of <span class="math inline">\(X_{i}\)</span>s, will tend to produce estimates closest to the true value?”</li>
</ul>
</section>
<section id="check-in" class="slide level2 smaller">
<h2>Check in</h2>
<p>Say whether each of the following is (a) a parameter or (b) an estimate.</p>
<ol type="1">
<li class="fragment">A study of survival in 1225 newly diagnosed breast cancer cases finds that survival varies greatly by stage of diagnosis. The average seven-year survival rates for Stage I breast cancer was 92%; the Stage II survival rate was 71%; the stage III survival rate was 39%; and the stage IV survival rate was 11%.</li>
<li class="fragment">A review of divorce records for a county in Connecticut indicates that the marriages that end in divorce last an average of 72 months.</li>
<li class="fragment">For U.S. men, the average life expectancy is 76. For women, it’s 81. [Life expectancy is based on data from the National Death Index; deaths are uniformly recorded in the U.S.]</li>
</ol>
</section>
<section id="mean-squared-error" class="slide level2 smaller">
<h2>Mean Squared Error</h2>
<p>How do we compare estimators? How do we know how close to the real value it is?</p>
<p>A popular way to quantify the idea of <span class="math inline">\(\hat{\theta}\)</span> being closed to <span class="math inline">\(\theta\)</span> is to consider the squared error <span class="math inline">\((\hat{\theta} - \theta)^{2}\)</span>. Another possibility is the absolute error <span class="math inline">\(|\hat{\theta} - \theta|\)</span>, but this is more difficult to work with mathematically. This value depends on the sample, so a common measure of accuracy is the mean squared error (expected squared error), which entails averaging the squared error over all possible samples and resulting estimates.</p>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>The <strong>mean squared error</strong> of an estimate <span class="math inline">\(\hat{\theta}\)</span> is <span class="math inline">\(E[(\hat{\theta} - \theta)^{2}]\)</span></p>
</div>
</div>
</div>
</section>
<section id="mean-squared-error-1" class="slide level2">
<h2>Mean Squared Error</h2>
<p>A useful result when evaluating mean squared error is shown below:</p>
<p><span class="math display">\[
MSE = V(\hat{\theta}) + [E(\hat{\theta}) - \theta]^{2} = \text{ variance of estimator + (bias)}^{2}
\]</span></p>
<p>where is defined as the difference between the expected value of the estimator and the parameter.</p>
</section>
<section id="bias" class="slide level2">
<h2>Bias</h2>

<img data-src="/Users/hollysteeves/Library/CloudStorage/OneDrive-TheUniversityofWesternOntario/SS2858/bias.png" alt="On the left: two symmetric densities, one of theta 1 hat, and one of theta two hat. Theta two hat is unbiased as it's centre is located at theta. The distance between the centre of theta 1 hat and theta is the bias of theta 1 hat. On the right: two right skewed densities, one of theta 2 hat and one of theta 1 hat. Theta 2 hat is unbiased as the location of the mean is at theta, whereas the distance between the mean of theta 1 hat and theta is the bias of theta 1 hat." class="r-stretch"></section>
<section id="example-20" class="slide level2">
<h2>Example 20</h2>
<p>Return to the problem of estimator the population proportion <span class="math inline">\(p\)</span> and consider another adjusted estimator, namely</p>
<p><span class="math display">\[
\hat{p} = \frac{X + \sqrt{n/4}}{n + \sqrt{n}}
\]</span></p>
<p>The justification for this estimator comes from the Bayesian approach to point estimator.</p>
<ol type="a">
<li class="fragment">Determine the mean squared error of this estimator. What do you find interesting about this MSE?</li>
<li class="fragment">Compare the MSE of this estimator to the MSE of the usual estimator (the sample proportion).</li>
</ol>
</section>
<section id="unbiased-estimators" class="slide level2">
<h2>Unbiased Estimators</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>A point estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be an <strong>unbiased estimator</strong> of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(E(\hat{\theta}) = \theta\)</span> for every possible value of <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(\hat{\theta}\)</span> is not unbiased, the difference <span class="math inline">\(E(\hat{\theta}) - \theta\)</span> is called the of <span class="math inline">\(\hat{\theta}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="unbiased-estimators-1" class="slide level2">
<h2>Unbiased Estimators</h2>
<p>That is, <span class="math inline">\(\hat{\theta}\)</span> is unbiased if its probability distribution is always “centered” at the true value of the parameter. Note that centered here means that the expected value, not the median, of the distribution of <span class="math inline">\(\hat{\theta}\)</span> = <span class="math inline">\(\theta\)</span>.</p>
</section>
<section id="unbiased-estimators-2" class="slide level2">
<h2>Unbiased Estimators</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Proposition</strong></p>
</div>
<div class="callout-content">
<p>When <span class="math inline">\(X\)</span> is a binomial RV with parameters <span class="math inline">\(n\)</span> and <span class="math inline">\(p\)</span>, the sample proportion <span class="math inline">\(\hat{p} = X/n\)</span> is an unbiased estimator of <span class="math inline">\(p\)</span>.</p>
</div>
</div>
</div>
<p>Let’s prove it!</p>
</section>
<section id="standard-deviation" class="slide level2">
<h2>Standard Deviation</h2>
<p>Recall:</p>
<p><span class="math display">\[
\sigma^{2} = \frac{\sum_{i=1}^{N}(X_{i} - \mu)^{2}}{N}
\]</span></p>
<p><span class="math display">\[
S^{2} = \frac{\sum_{i=1}^{n}(X_{i} - \bar{X})^{2}}{n-1}
\]</span></p>
<p>Ever wonder why we divide the population standard deviation by <span class="math inline">\(N\)</span>, but the sample standard deviation by <span class="math inline">\(n-1\)</span>? It’s so that the sample standard deviation is unbiased!!</p>
<p>Let’s prove it!</p>
</section>
<section id="unbiased-estimators-3" class="slide level2">
<h2>Unbiased Estimators</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Proposition</strong></p>
</div>
<div class="callout-content">
<p>If <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span> is a random sample from a distribution with mean <span class="math inline">\(\mu\)</span>, then <span class="math inline">\(\bar{X}\)</span> is an unbiased estimator of <span class="math inline">\(\mu\)</span>. If in addition the distribution is continuous and symmetric, then <span class="math inline">\(\tilde{X}\)</span> and any trimmed mean are also unbiased estimators of <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
</div>
<p>According to this proposition, the principle of unbiasedness by itself does not always allow us to select a single estimator. When the underlying population is normal, even the third estimator described above is unbiased, and there are many other unbiased estimates. What we now need is a way of selecting among unbiased estimators.</p>
</section>
<section id="example-10" class="slide level2">
<h2>Example 10</h2>
<p>Using a long rod that has length <span class="math inline">\(\mu\)</span>, you are going to lay out a square plot in which the length of each side is <span class="math inline">\(\mu\)</span>. Thus the area of the plot will be <span class="math inline">\(\mu^{2}\)</span>. However, you do not know the value of <span class="math inline">\(\mu\)</span>, so you decided to make <span class="math inline">\(n\)</span> independent measurements <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>,…,<span class="math inline">\(X_{n}\)</span> of the length. Assume that each <span class="math inline">\(X_{i}\)</span> has mean <span class="math inline">\(\mu\)</span> (unbiased measurements) and variance <span class="math inline">\(\sigma^{2}\)</span>.</p>
<ol type="a">
<li class="fragment">Show that <span class="math inline">\(\bar{X}^{2}\)</span> is not an unbiased estimate for <span class="math inline">\(\mu^{2}\)</span>. (Hint: for any RV <span class="math inline">\(Y\)</span>, <span class="math inline">\(E(Y^{2}) = V(Y) + E(Y)^{2}\)</span>. Apply this with <span class="math inline">\(Y=\bar{X}\)</span>.</li>
<li class="fragment">For what value of <span class="math inline">\(k\)</span> is the estimator <span class="math inline">\(\bar{X}^{2} - kS^{2}\)</span> unbiased for <span class="math inline">\(\mu^{2}\)</span>? (Hint: Compute <span class="math inline">\(E(\bar{X}^{2} - kS^{2})\)</span>)</li>
</ol>
</section>
<section id="estimators-with-minimum-variance" class="slide level2">
<h2>Estimators with Minimum Variance</h2>
<p>Suppose <span class="math inline">\(\hat{\theta}_{1}\)</span> and <span class="math inline">\(\hat{\theta}_{2}\)</span> are two estimators of <span class="math inline">\(\theta\)</span> that are both unbiased. Then, although the distribution of each estimator is centered at the true value of <span class="math inline">\(\theta\)</span>, the spreads of the distributions about the true value may be different.</p>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Principle of Minimum Variance Unbiased Estimation</strong></p>
</div>
<div class="callout-content">
<p>Among all estimators of <span class="math inline">\(\theta\)</span> that are unbiased, choose the one that has minimum variance. The resulting <span class="math inline">\(\hat{\theta}\)</span> is called the <strong>minimum variance unbiased estimator (MVUE)</strong> of <span class="math inline">\(\theta\)</span>. Since MSE = variance + (bias)<span class="math inline">\(^{2}\)</span>, seeking an unbiased estimator with minimum variance is the same as seeking an unbiased estimator that has minimum mean squared error.</p>
</div>
</div>
</div>
</section>
<section id="mvue" class="slide level2">
<h2>MVUE</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> be a random sample from a normal distribution with parameters <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma\)</span>. Then the estimator <span class="math inline">\(\hat{\mu} = \bar{X}\)</span> is the MVUE for <span class="math inline">\(\mu\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="cautions" class="slide level2 smaller">
<h2>Cautions</h2>
<p>The MVUE is not necessarily always “the best” estimator to use. The best estimator for <span class="math inline">\(\mu\)</span> depends crucially on which distribution is being sampled. In particular,</p>
<ol type="1">
<li class="fragment"><div class="callout callout-caution callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Danger</strong></p>
</div>
<div class="callout-content">
<p>If the random sample comes from a normal distribution, then <span class="math inline">\(\bar{X}\)</span> is the best of the four estimators, since it has minimum variance among all unbiased estimators.</p>
</div>
</div>
</div></li>
<li class="fragment"><div class="callout callout-caution callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Danger</strong></p>
</div>
<div class="callout-content">
<p>If the underlying distribution is the particular uniform distribution in 7.3, then the best estimator is <span class="math inline">\(\bar{X}_{e}\)</span>; in general, this estimator is greatly influenced by outlying observations, but here the lack of tails makes such observations impossible.</p>
</div>
</div>
</div></li>
<li class="fragment"><div class="callout callout-caution callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Danger</strong></p>
</div>
<div class="callout-content">
<p>The trimmed mean is best in none of the situations above, but works reasonably well in them. That is, <span class="math inline">\(\bar{X}_{tr(10)}\)</span> does not suffer too much in comparison with the best procedure in any of the situations described above.</p>
</div>
</div>
</div></li>
</ol>
</section>
<section id="reporting-a-point-estimate-the-standard-error" class="slide level2">
<h2>Reporting a Point Estimate: The Standard Error</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>The of an estimator <span class="math inline">\(\hat{\theta}\)</span> is its standard deviation <span class="math inline">\(\sigma_{\hat{\theta}} = \sqrt{V(\hat{\theta})}\)</span>. If the standard error itself involves unknown parameters whose values can be estimated, substitution of these estimates into <span class="math inline">\(\sigma_{\hat{\theta}}\)</span> yields the (estimated standard deviation) of the estimator. The estimated standard error can be denoted by either <span class="math inline">\(\hat{\sigma}_{\hat{\theta}}\)</span> (the <span class="math inline">\(\hat{}\)</span> over <span class="math inline">\(\sigma\)</span> emphasizes that <span class="math inline">\(\sigma_{\hat{\theta}}\)</span> is being estimated) or by <span class="math inline">\(s_{\hat{\theta}}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-4" class="slide level2 smaller">
<h2>Example 4</h2>
<p>Consider these third grade verbal IQ observations for males:</p>
<table>
<thead>
<tr class="header">
<th>117</th>
<th>103</th>
<th>121</th>
<th>112</th>
<th>120</th>
<th>132</th>
<th>113</th>
<th>117</th>
<th>132</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>149</td>
<td>125</td>
<td>131</td>
<td>136</td>
<td>107</td>
<td>108</td>
<td>113</td>
<td>136</td>
<td>114</td>
</tr>
</tbody>
</table>
<p>and females:</p>
<table>
<thead>
<tr class="header">
<th>114</th>
<th>102</th>
<th>113</th>
<th>131</th>
<th>124</th>
<th>117</th>
<th>120</th>
<th>90</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>114</td>
<td>109</td>
<td>102</td>
<td>114</td>
<td>127</td>
<td>127</td>
<td>103</td>
<td></td>
</tr>
</tbody>
</table>
<p>Prior to obtaining the data, denote the male values by <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>,…,<span class="math inline">\(X_{m}\)</span>, and the female values by <span class="math inline">\(Y_{1}\)</span>, <span class="math inline">\(Y_{2}\)</span>,…,<span class="math inline">\(Y_{n}\)</span>. Suppose that <span class="math inline">\(X_{i}\)</span>s constitute a random sample from a distribution with mean <span class="math inline">\(\mu_{1}\)</span> and standard deviation <span class="math inline">\(\sigma_{1}\)</span>, and that the <span class="math inline">\(Y_{i}\)</span>s form a random sample (independent of the <span class="math inline">\(X_{i}\)</span>s) from another distribution with mean <span class="math inline">\(\mu_{2}\)</span> and standard deviation <span class="math inline">\(\sigma_{2}\)</span>.</p>
</section>
<section id="example-4-1" class="slide level2 smaller">
<h2>Example 4</h2>
<ol type="a">
<li class="fragment">Use the rules of expected value to show that <span class="math inline">\(\bar{X} - \bar{Y}\)</span> is an unbiased estimator of <span class="math inline">\(\mu_{1} - \mu_{2}\)</span>. Calculate the estimate for the given data. </li>
<li class="fragment">Use rules of variance from Chapter 6 to obtain an expression for the variance and standard deviation (standard error) of the estimator in part (a), and then compute the estimated standard error. </li>
<li class="fragment">Calculate a point estimate of the ratio <span class="math inline">\(\sigma_{1}/\sigma_{2}\)</span> of the two standard deviations. </li>
<li class="fragment">Suppose one male third-grader and one female third-grader are randomly selected. Calculate a point estimate of the variance of the difference <span class="math inline">\(X - Y\)</span> between male and female IQ.</li>
</ol>
</section>
<section id="summary" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">A <strong>point estimate</strong> of a parameter <span class="math inline">\(\theta\)</span> is a single number that can be regarded as a sensible value for <span class="math inline">\(\theta\)</span> that is calculated from the given sample data.<br>
</li>
<li class="fragment">The selected statistic used to calculate a point estimate is called the <strong>point estimator</strong> of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">The <strong>mean squared error</strong> of an estimate <span class="math inline">\(\hat{\theta}\)</span> is <span class="math inline">\(E[(\hat{\theta} - \theta)^{2}]\)</span></li>
<li class="fragment">A point estimator <span class="math inline">\(\hat{\theta}\)</span> is said to be an <strong>unbiased estimator</strong> of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(E(\hat{\theta}) = \theta\)</span> for every possible value of <span class="math inline">\(\theta\)</span>.</li>
</ul>
</section>
<section id="summary-1" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">If <span class="math inline">\(\hat{\theta}\)</span> is not unbiased, the difference <span class="math inline">\(E(\hat{\theta}) - \theta\)</span> is called the <strong>bias</strong> of <span class="math inline">\(\hat{\theta}\)</span>.</li>
<li class="fragment">Among all estimators of <span class="math inline">\(\theta\)</span> that are unbiased, choose the one that has minimum variance. The resulting <span class="math inline">\(\hat{\theta}\)</span> is called the <strong>minimum variance unbiased estimator (MVUE)</strong> of <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">The <strong>standard error</strong> of an estimator <span class="math inline">\(\hat{\theta}\)</span> is its standard deviation <span class="math inline">\(\sigma_{\hat{\theta}} = \sqrt{V(\hat{\theta})}\)</span>.</li>
</ul>
</section>
<section id="practice-problems" class="slide level2">
<h2>Practice Problems</h2>
<ul>
<li class="fragment">Odd problems 1 - 17</li>
</ul>
</section></section>
<section>
<section id="methods-of-estimation" class="title-slide slide level1 center">
<h1>7.2 Methods of Estimation</h1>

</section>
<section id="outcomes" class="slide level2">
<h2>Outcomes</h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li class="fragment">Describe and solve for the method of moments estimator.</li>
<li class="fragment">Describe and solve for the maximum likelihood estimator.</li>
<li class="fragment">Solve for the MLE of a function of an unknown parameter.</li>
</ul>
</section>
<section id="the-method-of-moments" class="slide level2">
<h2>The Method of Moments</h2>
<p>The basic idea of this method is to equate certain sample characteristics, such as the mean, to the corresponding population expected values. Then solving these equations for unknown parameters yields the estimators.</p>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> be a random sample from a pmf or pdf <span class="math inline">\(f(x)\)</span>. For <span class="math inline">\(k = 1,2,3,...\)</span>, the <strong>k</strong><span class="math inline">\(^{th}\)</span> <strong>population moment</strong> or <strong>k</strong><span class="math inline">\(^{th}\)</span> <strong>moment of the distribution</strong> <span class="math inline">\(f(x)\)</span>, is <span class="math inline">\(E(X^{k})\)</span>. The <strong>k</strong><span class="math inline">\(^{th}\)</span> <strong>sample moment</strong> is <span class="math inline">\(\frac{1}{n}\sum_{i=1}^{n}X_{i}^{k}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="the-method-of-moments-1" class="slide level2">
<h2>The Method of Moments</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(X_{1}\)</span>,<span class="math inline">\(X_{2}\)</span>,…,<span class="math inline">\(X_{n}\)</span> be a random sample from a distribution with pmf or pdf <span class="math inline">\(f(x;\theta_{1},...,\theta_{m})\)</span> where <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span> are parameters whose values are unknown. Then the <strong>moment estimators</strong> <span class="math inline">\(\hat{\theta}_{1}\)</span>,…,<span class="math inline">\(\hat{\theta}_{m}\)</span> are obtained by equating the first <span class="math inline">\(m\)</span> sample moments to the corresponding first <span class="math inline">\(m\)</span> population moments and solving for <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-7.13" class="slide level2">
<h2>Example 7.13</h2>
<p>Let <span class="math inline">\(X_{1}\)</span>, …, <span class="math inline">\(X_{n}\)</span> represent a random sample of service times of <span class="math inline">\(n\)</span> customers at a certain facility, where the underlying distribution is assumed exponential with parameter <span class="math inline">\(\lambda\)</span>.</p>
<p>Since there is only one parameter to be estimated, the estimator is obtained by equating <span class="math inline">\(E(X)\)</span> to <span class="math inline">\(\bar{X}\)</span>. Since <span class="math inline">\(E(X) = 1/\lambda\)</span> for an exponential distribution, this gives</p>
<p><span class="math display">\[
1/\lambda = \bar{X} \text{ or } \lambda = 1/\bar{X}
\]</span></p>
<p>Therefore, the method of moments estimator is <span class="math inline">\(\hat{\lambda} = 1/\bar{X}\)</span></p>
</section>
<section id="example-7.14" class="slide level2">
<h2>Example 7.14</h2>
<p>Let <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> be a random sample from a gamma distribution with parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>. Find the method of moments estimators for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>.</p>
</section>
<section id="example-10-1" class="slide level2">
<h2>Example 10</h2>
<p>Suppose we have the following observations from a uniform distribution ranging from 0 to <span class="math inline">\(\theta\)</span> and we wish to estimate <span class="math inline">\(\theta\)</span> using the method of moments given that <span class="math inline">\(\bar{x} = 4.5375\)</span>.</p>
</section>
<section id="maximum-likelihood-estimation" class="slide level2">
<h2>Maximum Likelihood Estimation</h2>
<p>The method of maximum likelihood estimation was first introduced by R. A. Fisher, a geneticist and statistician in the 1920s. Most statisticians recommend this method at least when the sample size is large, since the resulting estimators have certain desirable efficiency properties.</p>
</section>
<section id="maximum-likelihood-estimation-1" class="slide level2">
<h2>Maximum Likelihood Estimation</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> have joint pmf or pdf <span class="math inline">\(f(x_{1},x_{2},...,x_{n};\theta_{1},...,\theta_{m})\)</span> where the parameters <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span> have unknown values. When <span class="math inline">\(x_{1},...,x_{n}\)</span> are the observed sample values and the joint pmf or pdf above is regarded as a function of <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span>, it is called the <strong>likelihood function</strong>. The maximum likelihood estimates <span class="math inline">\(\hat{\theta}_{1}\)</span>,…,<span class="math inline">\(\hat{\theta}_{m}\)</span> are those values of the <span class="math inline">\(\theta_{i}\)</span>s that maximize the likelihood function, so that</p>
<p><span class="math display">\[
f(x_{1}, x_{2},...,x_{n}; \hat{\theta}_{1},...,\hat{\theta}_{1}) \geq f(x_{1},x_{2},...,x_{n};\theta_{1},...,\theta_{m})
\]</span> for all <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span>.</p>
<p>When the <span class="math inline">\(X_{i}\)</span>s are substituted in place of the <span class="math inline">\(x_{i}\)</span>s, the <strong>maximum likelihood estimators</strong> (mles) result.</p>
</div>
</div>
</div>
</section>
<section id="mle-example" class="slide level2 smaller">
<h2>MLE Example</h2>
<p>Suppose <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> is a random sample from an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>. Because of independence, the likelihood function is a product of the individual pdf’s.</p>
<p><span class="math display">\[
f(x_{1},...,x_{n}; \lambda) = (\lambda e^{\lambda x_{1}})(\lambda e^{\lambda x_{2}})\cdots (\lambda e^{\lambda x_{n}}) \ \lambda^{n} e^{-\lambda\sum x_{i}}
\]</span></p>
<p>The ln(likelihood) is</p>
<p><span class="math display">\[
\ln[f(x_{1},...,x_{n}; \lambda)] = n\ln(\lambda) - \lambda\sum x_{i}
\]</span></p>
<p>Equating <span class="math inline">\((d/d\lambda)\)</span>(ln[likelihood]) to zero results in <span class="math inline">\(n/\lambda - \sum x_{i} = 0\)</span> or <span class="math inline">\(\lambda = n/\sum x_{i} = 1/\bar{x}\)</span>; it is identical to the method of moments estimator but it is not an unbiased estimator, since <span class="math inline">\(E(1/\bar{X}) \neq 1/E(\bar{X})\)</span>.</p>
</section>
<section id="example-30" class="slide level2 smaller">
<h2>Example 30</h2>
<p>Consider a random sample <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> from the shifted exponential pdf</p>
<p><span class="math display">\[
f(x; \lambda, \theta) = \begin{cases}
\lambda e^{-\lambda(x-\theta)} &amp; x\geq \theta \\
0 &amp; \text{otherwise}
\end{cases}
\]</span></p>
<p>Taking <span class="math inline">\(\lambda = 0\)</span> gives the pdf of the exponential distribution considered previously. An example of the shifted exponential distribution appeared in Example 4.5 in which the variable of interest was time headway in traffic flow and <span class="math inline">\(\theta = 0.5\)</span> was the minimum possible time headway.</p>
<ol type="a">
<li class="fragment">Obtain the maximum likelihood estimators of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\theta\)</span>.</li>
<li class="fragment">If <span class="math inline">\(n=10\)</span> time headway observations are made, resulting in the values 3.11, 0.64, 2.55, 2.20, 5.44, 3.42, 10.39, 8.93, 17.82, and 1.30, calculate the estimates of <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\theta\)</span>.</li>
</ol>
</section>
<section id="some-properties-of-mles" class="slide level2">
<h2>Some Properties of MLEs</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The Invariance Principle</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(\hat{\theta}_{1}\)</span>,…,<span class="math inline">\(\hat{\theta}_{m}\)</span> be the MLEs of the parameters <span class="math inline">\(\theta_{1}\)</span>,…,<span class="math inline">\(\theta_{m}\)</span>. Then the MLE of any function <span class="math inline">\(h(\theta_{1},...,\theta_{m})\)</span> of these parameters is the function <span class="math inline">\(h(\hat{\theta}_{1},...,\hat{\theta}_{m})\)</span>, of the MLEs.</p>
</div>
</div>
</div>
</section>
<section id="example-the-invariance-principle" class="slide level2">
<h2>Example: The Invariance Principle</h2>
<p>It can be shown that in the case of a normal distribution, the MLE of <span class="math inline">\(\mu\)</span> is <span class="math inline">\(\bar{X}\)</span>. Now, what is the MLE of <span class="math inline">\(\sigma = h(\mu) = \sqrt{\frac{1}{n}\sum (X_{i} - \mu)^{2}}\)</span>? From the invariance principle,</p>
<p><span class="math display">\[
\hat{\sigma} = h(\bar{X}) = \sqrt{\frac{1}{n}\sum (X_{i} - \bar{X})^{2}}
\]</span></p>
<p>Note: This is not the sample standard deviation <span class="math inline">\(S\)</span>, although they are close unless <span class="math inline">\(n\)</span> is quite small.</p>
</section>
<section id="large-sample-behaviour-of-the-mle" class="slide level2">
<h2>Large-Sample Behaviour of the MLE</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Proposition</strong></p>
</div>
<div class="callout-content">
<p>Under very general conditions on the joint distribution of the sample, when the sample size is large, the maximum likelihood estimator of ay parameter <span class="math inline">\(\theta\)</span> is close to <span class="math inline">\(\theta\)</span> (consistency), is approximately unbiased (<span class="math inline">\(E[\hat{\theta}] \approx \theta\)</span>), and has variance that is nearly as small as can be achieved by any unbiased estimator. Stated another way, the mle <span class="math inline">\(\hat{\theta}\)</span> is approximately the MVUE of <span class="math inline">\(\theta\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="large-sample-behaviour-of-the-mle-1" class="slide level2">
<h2>Large-Sample Behaviour of the MLE</h2>
<p>Because of this result and the fact that calculus-based techniques can usually be used to derive the MLEs (although often numerical methods, such as Newton’s method, are necessary), maximum likelihood estimator is the most widely used estimation technique among statisticians. Many of the estimators used in the remainder of the book are MLEs. Obtaining an MLE, however, does require that the underlying distribution be specified.</p>
</section>
<section id="summary-2" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">Method of Moments estimators are found by equating the population and sample moments, and solving for the unknown parameters.</li>
<li class="fragment">Maximum likelihood estimators are found by defining the likelihood (the density written as a function of the parameters) and finding the value of the parameter at which the likelihood is a maximum.</li>
<li class="fragment">The MLE of any function of parameters is the function evaluated at the MLEs of the parameters.</li>
<li class="fragment">The MLE is approximately the MVUE.</li>
</ul>
</section>
<section id="practice-problems-1" class="slide level2">
<h2>Practice Problems</h2>
<ul>
<li class="fragment">Odd problems 21 - 29</li>
</ul>
</section></section>
<section>
<section id="sufficiency" class="title-slide slide level1 center">
<h1>7.3 Sufficiency</h1>

</section>
<section id="outcomes-1" class="slide level2">
<h2>Outcomes</h2>
<p>By the end of this section, you will be able to</p>
<ul>
<li class="fragment">Define a sufficient statistic and determine if a given statistic is sufficient.</li>
<li class="fragment">Define jointly sufficient statistics and determine if given statistics are jointly sufficient.</li>
<li class="fragment">Define a minimal sufficient statistic and determine if a given statistic is minimally sufficient.</li>
</ul>
</section>
<section id="motivation" class="slide level2">
<h2>Motivation</h2>
<p>An investigation of major defects on new vehicles of a certain type involved selected a random sample of <span class="math inline">\(n=3\)</span> vehicles and determining for each one the value of <span class="math inline">\(X\)</span> = the number of major defects. This resulted in observations <span class="math inline">\(x_{1} = 1\)</span>, <span class="math inline">\(x_{2} = 0\)</span>, and <span class="math inline">\(x_{3} = 3\)</span>. You, as a consulting statistician have been provided with a description of the experiment, from which it is reasonable to assume that <span class="math inline">\(X\)</span> has a poisson distribution, and told only that the total number of defects for the three sampled vehicles was four.</p>
</section>
<section id="motivation-1" class="slide level2 smaller">
<h2>Motivation</h2>
<p>Knowing that <span class="math inline">\(T = \sum X_{i} = 4\)</span>, would there be any additional advantage in having the observed values of the individual <span class="math inline">\(X_{i}\)</span>s when making an inference about the Poisson parameter <span class="math inline">\(\lambda\)</span>? Or rather is it the case that the statistic <span class="math inline">\(T\)</span> contains all relevant information about <span class="math inline">\(\lambda\)</span> in the data? To address this issue, consider the conditional distribution of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, <span class="math inline">\(X_{3}\)</span> given that <span class="math inline">\(\sum X_{i} = 4\)</span>. Fist of all, there are a few possible <span class="math inline">\((x_{1}, x_{2}, x_{3})\)</span> triples for which <span class="math inline">\(x_{1} + x_{2} + x_{3} = 4\)</span>. For example, <span class="math inline">\((0, 4, 0)\)</span> is a possibility, as are <span class="math inline">\((2, 2, 0)\)</span> and <span class="math inline">\((1, 0, 3)\)</span>, but not <span class="math inline">\((1, 2, 3)\)</span> or <span class="math inline">\((5, 0, 2)\)</span>. That is:</p>
<p><span class="math display">\[
P(X_{1} = x_{1}, X_{2} = x_{2}, X_{3} = x_{3}| \sum_{i=1}^{3}X_{i} = 4) = 0
\]</span> unless <span class="math inline">\(x_{1} + x_{2} + x_{3} = 4\)</span>.</p>
</section>
<section id="motivation-2" class="slide level2 smaller">
<h2>Motivation</h2>
<p>Now consider the triple <span class="math inline">\((2, 1, 1)\)</span>, which is consistent with <span class="math inline">\(\sum X_{i} = 4\)</span>. If we let <span class="math inline">\(A\)</span> denote the event that <span class="math inline">\(X_{1} = 2\)</span>, <span class="math inline">\(X_{2} = 1\)</span>, and <span class="math inline">\(X_{3} = 1\)</span>, and <span class="math inline">\(B\)</span> denote the event that <span class="math inline">\(\sum X_{i} = 4\)</span>, then the event <span class="math inline">\(A\)</span> implies the event <span class="math inline">\(B\)</span>, so the intersection of the two events is just the smaller event <span class="math inline">\(A\)</span>. Thus:</p>
<p><span class="math display">\[
\begin{aligned}
P(X_{1} = 2, X_{2} = 1, X_{3} = 1)|\sum_{i=1}^{3}X_{i} = 4) &amp;= P(A|B) \\
&amp;= \frac{P(A\cap B)}{P(B)}\\
&amp;= \frac{P(X_{1} = 2, X_{2}= 1, X_{3} =)}{P(\sum X_{i} = 4)}
\end{aligned}
\]</span></p>
</section>
<section id="motivation-3" class="slide level2 smaller">
<h2>Motivation</h2>
<p>A moment generating function argument shows that <span class="math inline">\(\sum X_{i}\)</span> has a Poisson distribution with parameter <span class="math inline">\(3\lambda\)</span>. Thus, the desired conditional probability is:</p>
<p><span class="math display">\[
\frac{\frac{e^{-\lambda}\lambda^{2}}{2!}\frac{e^{-\lambda}\lambda^{1}}{1!}\frac{e^{-\lambda}\lambda^{1}}{1!}}{\frac{e^{-3\lambda}(3\lambda)^{4}}{4!}} = \frac{4!}{3^{4}\cdot 2!} = 4/27
\]</span> Similarly, this can be done for all possible values of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, and <span class="math inline">\(X_{3}\)</span> satisfying <span class="math inline">\(X_{1} + X_{2} + X_{3} = 4\)</span>. Neither this conditional probability, nor any of the other conditional probabilities involve <span class="math inline">\(\lambda\)</span>. Thus, once the value of the statistic <span class="math inline">\(\sum X_{i}\)</span> has been provided, there is no additional information about <span class="math inline">\(\lambda\)</span> in the individual observations.</p>
</section>
<section id="sufficiency-1" class="slide level2">
<h2>Sufficiency</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>A statistic <span class="math inline">\(T = t(X_{1},X_{2},...,X_{n})\)</span> is said to be <strong>sufficient</strong> for making inferences about a parameter <span class="math inline">\(\theta\)</span> if the joint distribution of <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> given that <span class="math inline">\(T = t\)</span> does not depend upon <span class="math inline">\(\theta\)</span> for every possible value <span class="math inline">\(t\)</span> of the statistic <span class="math inline">\(T\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="sufficiency-2" class="slide level2">
<h2>Sufficiency</h2>
<p>How can a sufficient statistic be identified? It may seem as though one would have to select a statistic and determine the conditional distribution of the <span class="math inline">\(X_{i}\)</span>s given any particular value of the statistic. This would be terribly time-consuming, and when the <span class="math inline">\(X_{i}\)</span>s are continuous there are additional technical difficulties in obtaining the relevant conditional distribution. Fortunately, the next result provides a relatively straightforward way of proceeding.</p>
</section>
<section id="sufficiency-3" class="slide level2">
<h2>Sufficiency</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>The Neyman Factorization Theorem</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(f(x_{1}, x_{2}, ..., x_{n}; \theta)\)</span> denote the joint pmf or pdf of <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>. Then <span class="math inline">\(T = t(X_{1}, ..., X_{n})\)</span> is a sufficient statistic for <span class="math inline">\(\theta\)</span> if and only if the joint pmf or pdf can be represented as a product of two factors in which the first factor involves <span class="math inline">\(\theta\)</span> and the data only through <span class="math inline">\(t(x_{1}, ..., x_{n})\)</span> whereas the second factor involves <span class="math inline">\(x_{1}\)</span>, …, <span class="math inline">\(x_{n}\)</span> but does not depend on <span class="math inline">\(\theta\)</span>:</p>
<p><span class="math display">\[
f(x_{1}, x_{2}, ..., x_{n}; \theta) = g(t(x_{1}, ..., x_{n}); \theta)\cdot h(x_{1}, ..., x_{n})
\]</span></p>
</div>
</div>
</div>
</section>
<section id="sufficiency-example" class="slide level2">
<h2>Sufficiency Example</h2>
<p>Let’s generalize the previous example by considering a random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span> from a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>, for example, the numbers of blemishes on <span class="math inline">\(n\)</span> independently selected DVD’s or the numbers of errors in <span class="math inline">\(n\)</span> batches of invoices where each batch consists of 200 invoices. The joint pmf of these variables is</p>
</section>
<section id="sufficiency-example-1" class="slide level2">
<h2>Sufficiency Example</h2>
<p><span class="math display">\[
\begin{aligned}
f(x_{1},...,x_{n};\lambda) = \frac{e^{-\lambda}\lambda^{x_{1}}}{x_{1}!}\cdot \frac{e^{-\lambda}\lambda^{x_{2}}}{x_{2}!}\cdots \frac{e^{-\lambda}\lambda^{x_{n}}}{x_{n}!} &amp;= \frac{e^{-n\lambda}\lambda^{x_{1}+x_{2}+\cdots + x_{n}}}{x_{1}!x_{2}!\cdots x_{n}!} \\
&amp;= (e^{-n\lambda}\lambda^{\sum x_{i}})\left(\frac{1}{x_{1}!x_{2}!\cdots x_{n}!}\right)
\end{aligned}
\]</span></p>
<p>The factor inside the first set of parentheses involves the parameter <span class="math inline">\(\lambda\)</span> and the data only through <span class="math inline">\(\sum x_{i}\)</span>, whereas the factor inside the second set of parentheses involves the data but not <span class="math inline">\(\lambda\)</span>. So we have the desired factorization, and the sufficient statistic <span class="math inline">\(T = \sum X_{i}\)</span>, as we previously ascertained directly from the definition of sufficiency.</p>
</section>
<section id="sufficiency-4" class="slide level2">
<h2>Sufficiency</h2>
<p>Note: A sufficient statistic is not unique; any one-to-one function of a sufficient statistic is itself sufficient. In the Poisson example, the sample mean <span class="math inline">\(\bar{X} = \frac{1}{n}\sum X_{i}\)</span> is a one-to-one function of <span class="math inline">\(\sum X_{i}\)</span> (knowing the value of the sum of the <span class="math inline">\(n\)</span> observations is equivalent to knowing their mean), so the sample mean is also a sufficient statistic.</p>
</section>
<section id="example-34" class="slide level2">
<h2>Example 34</h2>
<p>Let <span class="math inline">\(X_{1}\)</span>, …, <span class="math inline">\(X_{n}\)</span> be a random sample of component lifetimes from an exponential distribution with parameter <span class="math inline">\(\lambda\)</span>. Use the factorization theorem to show that <span class="math inline">\(\sum X_{i}\)</span> is a sufficient statistic for <span class="math inline">\(\lambda\)</span>.</p>
</section>
<section id="jointly-sufficient-statistics" class="slide level2">
<h2>Jointly Sufficient Statistics</h2>
<p>When the joint pmf or pdf of the data involves a single unknown parameter <span class="math inline">\(\theta\)</span>, there is frequently a single statistic (single function of the data) that is sufficient. However, when there are several unknown parameters—for example, the mean <span class="math inline">\(\mu\)</span> and standard deviation <span class="math inline">\(\sigma\)</span> of a normal distribution, or the shape parameter <span class="math inline">\(\alpha\)</span> and scale parameter <span class="math inline">\(\beta\)</span> of a gamma distribution—we must expand our notion of sufficiency.</p>
</section>
<section id="jointly-sufficient" class="slide level2">
<h2>Jointly Sufficient</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Suppose the joint pmf or pdf of the data involves <span class="math inline">\(k\)</span> unknown parameters <span class="math inline">\(\theta_{1}\)</span>, <span class="math inline">\(\theta_{2}\)</span>, …, <span class="math inline">\(\theta_{k}\)</span>. The <span class="math inline">\(m\)</span> statistics <span class="math inline">\(T_{1} = t_{1}(X_{1}, ..., X_{n})\)</span>, <span class="math inline">\(T_{2} = t_{2}(X_{1}, ..., X_{n})\)</span>, …, <span class="math inline">\(T_{m} = t_{m}(X_{1}, ..., X_{n})\)</span> are said to be jointly sufficient for the parameters if the conditional distribution of the <span class="math inline">\(X_{i}\)</span>s given that <span class="math inline">\(T_{1} = t_{1}\)</span>, <span class="math inline">\(T_{2} = t_{2}\)</span>, …, <span class="math inline">\(T_{m} = t_{m}\)</span> does not depend on any of the unknown parameters, and this is true for all possible values <span class="math inline">\(t_{1}\)</span>, <span class="math inline">\(t_{2}\)</span>, . . ., <span class="math inline">\(t_{m}\)</span> of the statistics.</p>
</div>
</div>
</div>
</section>
<section id="example-36" class="slide level2">
<h2>Example 36</h2>
<p>Suppose waiting time for delivery of an item is uniform on the interval from <span class="math inline">\(\theta_{1}\)</span> to <span class="math inline">\(\theta_{2}\)</span> (so <span class="math inline">\(f(x; \theta_{1}, \theta_{2}) = 1/(\theta_{2} - \theta_{1})\)</span> for <span class="math inline">\(\theta_{1} &lt; x &lt; \theta_{2}\)</span> and is <span class="math inline">\(0\)</span> otherwise). Consider a random sample of <span class="math inline">\(n\)</span> waiting times, and use the factorization theorem to show that <span class="math inline">\(min(X_{i})\)</span>, <span class="math inline">\(max(X_{i})\)</span> is a pair of jointly sufficient statistics for <span class="math inline">\(\theta_{1}\)</span> and <span class="math inline">\(\theta_{2}\)</span>. [Hint: Introduce an appropriate indicator function.]</p>
</section>
<section id="minimal-sufficiency" class="slide level2 smaller">
<h2>Minimal Sufficiency</h2>
<p>When <span class="math inline">\(X_{1}\)</span>, …, <span class="math inline">\(X_{n}\)</span> constitute a random sample from a normal distribution, the <span class="math inline">\(n\)</span> order statistics <span class="math inline">\(Y_{1}\)</span>, …, <span class="math inline">\(Y_{n}\)</span> are jointly sufficient for <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^{2}\)</span>, and the sample mean and sample variance are also jointly sufficient. Both the order statistics and the pair <span class="math inline">\((\bar{X}; S^{2})\)</span> reduce the data without any information loss, but the sample mean and variance represent a greater reduction. In general, we would like the greatest possible reduction without information loss. A minimal (possibly jointly) sufficient statistic is a function of every other sufficient statistic. That is, given the value(s) of any other sufficient statistic(s), the value(s) of the minimal sufficient statistic(s) can be calculated. The minimal sufficient statistic is the sufficient statistic having the smallest dimensionality, and thus represents the greatest possible reduction of the data without any information loss.</p>
</section>
<section id="improving-an-estimator" class="slide level2">
<h2>Improving an Estimator</h2>
<p>Because a sufficient statistic contains all the information the data has to offer about the value of <span class="math inline">\(\theta\)</span>, it is reasonable that an estimator of <span class="math inline">\(\theta\)</span> or any function of <span class="math inline">\(\theta\)</span> should depend on the data only through the sufficient statistic. A general result due to Rao and Blackwell shows how to start with an unbiased statistic that is not a function of sufficient statistics and create an improved estimator that is sufficient.</p>
</section>
<section id="improving-an-estimator-1" class="slide level2">
<h2>Improving an Estimator</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>Suppose that the joint distribution of <span class="math inline">\(X_{1}\)</span>, …, <span class="math inline">\(X_{n}\)</span> depends on some unknown parameter <span class="math inline">\(\theta\)</span> and that <span class="math inline">\(T\)</span> is sufficient for <span class="math inline">\(\theta\)</span>. Consider estimating <span class="math inline">\(h(\theta)\)</span>, a specified function of <span class="math inline">\(\theta\)</span>. If <span class="math inline">\(U\)</span> is an unbiased statistic for estimating <span class="math inline">\(h(\theta)\)</span> that does not involve <span class="math inline">\(T\)</span>, then the estimator <span class="math inline">\(U^{*} = E(U | T)\)</span> is also unbiased for <span class="math inline">\(h(\theta)\)</span> and has variance no greater than the original unbiased estimator <span class="math inline">\(U\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-39" class="slide level2 smaller">
<h2>Example 39</h2>
<p>The probability that any particular component of a certain type works in a satisfactory manner is <span class="math inline">\(p\)</span>. If <span class="math inline">\(n\)</span> of these components are independently selected, then the statistic <span class="math inline">\(X\)</span>, the number among the selected components that perform in a satisfactory manner, is sufficient for <span class="math inline">\(p\)</span>. You must purchase two of these components for a particular system. Obtain an unbiased statistic for the probability that exactly one of your purchased components will perform in a satisfactory manner.</p>
<p>[Hint: Start with the statistic U, the indicator function of the event that exactly one of the first two components in the sample of size <span class="math inline">\(n\)</span> performs as desired, and improve on it by conditioning on the sufficient statistic.]</p>
</section>
<section id="another-bonus-to-mle" class="slide level2">
<h2>Another Bonus to MLE</h2>
<p>If <span class="math inline">\(T_{1}\)</span>, …, <span class="math inline">\(T_{m}\)</span> are jointly sufficient statistics for parameters <span class="math inline">\(\theta_{1}\)</span>, …, <span class="math inline">\(y_{k}\)</span>, then the joint pmf or pdf factors as follows:</p>
<p><span class="math display">\[
f(x_{1}, ..., x_{n}; \theta_{1}, ..., \theta_{k}) = g(t_{1}, ..., t_{m}; \theta_{1}; ...; \theta_{k})\cdot h(x_{1}, ..., x_{n})
\]</span></p>
<p>The maximum likelihood estimates result from maximizing <span class="math inline">\(f(\cdot)\)</span> with respect to the <span class="math inline">\(\theta_{i}\)</span>s. Because the <span class="math inline">\(h(\cdot)\)</span> factor does not involve the parameters, this is equivalent to maximizing the <span class="math inline">\(g(\cdot)\)</span> factor with respect to the <span class="math inline">\(\theta_{i}\)</span>s. The resulting <span class="math inline">\(\hat{\theta}_{i}\)</span>s will involve the data only through the <span class="math inline">\(t_{i}\)</span>s. Thus it is always possible to find a maximum likelihood estimator that is a function of just the sufficient statistic(s).</p>
</section>
<section id="summary-3" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">A statistic is for making inferences about a parameter if the joint distribution of <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> given that <span class="math inline">\(T = t\)</span> does not depend upon <span class="math inline">\(\theta\)</span> for every possible value <span class="math inline">\(t\)</span> of the statistic <span class="math inline">\(T\)</span>.</li>
<li class="fragment">Any one-to-one function of a sufficient statistic is itself sufficient.</li>
<li class="fragment">The <span class="math inline">\(m\)</span> statistics <span class="math inline">\(T_{1} = t_{1}(X_{1}, ..., X_{n})\)</span>, <span class="math inline">\(T_{2} = t_{2}(X_{1}, ..., X_{n})\)</span>, …, <span class="math inline">\(T_{m} = t_{m}(X_{1}, ..., X_{n})\)</span> are said to be jointly sufficient for the parameters if the conditional distribution of the <span class="math inline">\(X_{i}\)</span>s given that <span class="math inline">\(T_{1} = t_{1}\)</span>, <span class="math inline">\(T_{2} = t_{2}\)</span>, …, <span class="math inline">\(T_{m} = t_{m}\)</span> does not depend on any of the unknown parameters, and this is true for all possible values <span class="math inline">\(t_{1}\)</span>, <span class="math inline">\(t_{2}\)</span>, . . ., <span class="math inline">\(t_{m}\)</span> of the statistics.</li>
</ul>
</section>
<section id="summary-4" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">The minimal sufficient statistic is the sufficient statistic having the smallest dimensionality, and thus represents the greatest possible reduction of the data without any information loss.</li>
<li class="fragment">If <span class="math inline">\(U\)</span> is an unbiased statistic for estimating <span class="math inline">\(h(\theta)\)</span> that does not involve <span class="math inline">\(T\)</span>, then the estimator <span class="math inline">\(U^{*} = E(U | T)\)</span> is also unbiased for <span class="math inline">\(h(\theta)\)</span> and has variance no greater than the original unbiased estimator <span class="math inline">\(U\)</span>.</li>
<li class="fragment">It is always possible to find a maximum likelihood estimator that is a function of just the sufficient statistic(s).</li>
</ul>
</section>
<section id="practice-problems-2" class="slide level2">
<h2>Practice Problems</h2>
<ul>
<li class="fragment">Odd Problems 33 - 39</li>
</ul>
</section>
<section id="outcomes-2" class="slide level2">
<h2>Outcomes</h2>
<p>By the end of this section, you will be able to:</p>
<ul>
<li class="fragment">Define and calculate Fisher’s Information.</li>
<li class="fragment">Define and calculate the Score Function.</li>
<li class="fragment">Calculate Fisher’s Information for a random sample</li>
<li class="fragment">Calculate the Cramer-Rao lower bound for a statistic.</li>
<li class="fragment">Define and calculate efficiency.</li>
<li class="fragment">Define and determine if a statistic is an efficient statistic.<br>
</li>
<li class="fragment">Understand the limiting properties of a maximum likelihood estimator.</li>
</ul>
</section></section>
<section>
<section id="information-and-efficiency" class="title-slide slide level1 center">
<h1>7.4 Information and Efficiency</h1>

</section>
<section id="information" class="slide level2 smaller">
<h2>Information</h2>
<p>In this section we introduce the idea of Fisher information and two of its applications. The first application is to find the minimum possible variance for an unbiased estimator. The second appliation is to show that the maximum likelihood estimator is asymptotically unbiased and normal (that is, for large <span class="math inline">\(n\)</span> it has expected value approximately <span class="math inline">\(\theta\)</span> and it has approximately a normal distribution) with the minimum possible variance.</p>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>The <strong>Fisher information</strong> <span class="math inline">\(I(\theta)\)</span> <strong>in a single observation from a pmf or pdf</strong> <span class="math inline">\(f(x;\theta)\)</span> is the variance of the random variable <span class="math inline">\(U = \partial[\ln[f(X;\theta)]]/\partial\theta\)</span>:</p>
<p><span class="math display">\[
I(\theta) = V\left[ \frac{\partial}{\partial\theta}\ln(f(X;\theta))\right]
\]</span></p>
</div>
</div>
</div>
</section>
<section id="fishers-information" class="slide level2">
<h2>Fisher’s Information</h2>
<p>There is an alternative expression for <span class="math inline">\(I(\theta)\)</span> that is sometimes easier to compute than the variance in the definition:</p>
<p><span class="math display">\[
I(\theta) = -E\left[ \frac{\partial^{2}}{\partial\theta^{2}}\ln f(X;\theta)\right]
\]</span></p>
</section>
<section id="example" class="slide level2 smaller">
<h2>Example</h2>
<p>Let <span class="math inline">\(X\)</span> be a Bernoulli rv, so <span class="math inline">\(f(x; p) = p^{x}(1-p)^{1-x}\)</span> <span class="math inline">\(x = 0,1\)</span> , <span class="math inline">\(x = 0, 1\)</span>. Then</p>
<p><span class="math display">\[
\frac{\partial}{\partial p}\ln(f(X;p)) = \frac{\partial}{\partial p}[X\ln p + (1-X)\ln(1-p)] = \frac{X}{p} - \frac{1-X}{1-p} = \frac{X-p}{p(1-p)}
\]</span></p>
<p>This has mean <span class="math inline">\(0\)</span> because <span class="math inline">\(E(X) = p\)</span>. Computing the variance of the partial derivative, we get the Fisher information:</p>
<p><span class="math display">\[
\begin{aligned}
I(p) &amp;= V\left[\frac{\partial}{\partial p} \ln(f(X;p))\right] \\
&amp;= \frac{V(X-p)}{[p(1-p)]^{2}} = \frac{V(X)}{[p(1-p)]^{2}} \\
&amp;= \frac{p(1-p)}{[p(1-p)]^{2}} = \frac{1}{p(1-p)}
\end{aligned}
\]</span></p>
</section>
<section id="fisher-information-example" class="slide level2">
<h2>Fisher Information Example</h2>
<p>The alternative method is as follows:</p>
<p><span class="math display">\[
\begin{aligned}
\frac{\partial^{2}}{\partial p^{2}}\ln(f(X;p)) = \frac{\partial}{\partial p} \left(\frac{X-p}{p(1-p)}\right) &amp;= \frac{-1(p(1-p)) - (X-p)(1-2p)}{[p(1-p)]^{2}} \\
&amp;= \frac{-p + p^{2} - X + 2Xp + p - 2p^{2}}{[p(1-p)]^{2}}\\
&amp;= \frac{-X + 2pX -p^{2}}{p^{2}(1-p)^{2}}
\end{aligned}
\]</span></p>
</section>
<section id="fishers-information-example" class="slide level2">
<h2>Fisher’s Information Example</h2>
<p>Taking the negative of the expected value gives the information in an observation:</p>
<p><span class="math display">\[
\begin{aligned}
I(p) = -E\left[ \frac{\partial{2}}{\partial p^{2}} \ln(f(X;p))\right] &amp;= \frac{-1}{p^{2}(1-p)^{2}}(-E(X) + 2pE(X) - p^{2}) \\
&amp;= \frac{-1}{p^{2}(1-p)^{2}} (-p + 2p^{2} - p^{2}) \\
&amp;= \frac{-1}{p^{2}(1-p)^{2}}(-p(1-p)) \\
&amp;= \frac{1}{p(1-p)}
\end{aligned}
\]</span></p>
</section>
<section id="example-42" class="slide level2">
<h2>Example 42</h2>
<p>Assume that the number of defects in a car has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. To estimate <span class="math inline">\(\lambda\)</span> we obtain the random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>.</p>
<ol type="a">
<li class="fragment">Find the Fisher information in a single observation using two methods.</li>
</ol>
</section>
<section id="information-in-a-random-sample" class="slide level2">
<h2>Information in a Random Sample</h2>
<p>Now assume a random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span> from a distribution with pmf or pdf <span class="math inline">\(f(x; \theta)\)</span>. Let <span class="math inline">\(f(X_{1}, X_{2}, ..., X_{n}; \theta) = f(X_{1}; \theta) \cdot f(X_{2}; \theta) \cdots f(X_{n}; \theta)\)</span> be the likelihood function. The Fisher information <span class="math inline">\(I_{n}(\theta)\)</span> for the random sample is the variance of the <strong>score function</strong>.</p>
<p><span class="math display">\[
\frac{\partial}{\partial\theta}\ln f(X_{1}, X_{2},...,X_{n};\theta) = \frac{\partial}{\partial\theta}\ln[f(X_{1};\theta)\cdot f(X_{2};\theta)\cdots f(X_{n};\theta)]
\]</span></p>
</section>
<section id="information-in-a-random-sample-1" class="slide level2">
<h2>Information in a Random Sample</h2>
<p>Using properties of logs and expected values, we find that:</p>
<p><span class="math display">\[
I_{n}(\theta) = V\left[\frac{\partial}{\partial\theta}\ln(f(X_{1}, X_{2},...,X_{n};\theta))\right] = nV\left[\frac{\partial}{\partial\theta}\ln f(X_{1};\theta) \right] = nI(\theta)
\]</span></p>
<p>Therefore, the Fisher information in a random sample is just <span class="math inline">\(n\)</span> times the information in a single observation. This should make sense intuitively, because it says that twice as many observations yield twice as much information.</p>
</section>
<section id="the-cramer-rao-inequality" class="slide level2">
<h2>The Cramer-Rao Inequality</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>Assume a random sample <span class="math inline">\(X_{1}\)</span>,…,<span class="math inline">\(X_{n}\)</span> from the distribution with pmf or pdf <span class="math inline">\(f(x;\theta)\)</span> such that the set of possible values does not depend on <span class="math inline">\(\theta\)</span>. If the statistic <span class="math inline">\(T = t(X_{1},...,X_{n})\)</span> is an unbiased estimator for the parameter <span class="math inline">\(\theta\)</span>, then</p>
<p><span class="math display">\[
V(T) \geq \frac{1}{V\{\frac{\partial}{\partial\theta}[\ln f(X_{1},...,X_{n};\theta)]\}} = \frac{1}{nI(\theta)} = \frac{1}{I_{n}(\theta)}
\]</span></p>
</div>
</div>
</div>
</section>
<section id="example-42-1" class="slide level2">
<h2>Example 42</h2>
<p>Assume that the number of defects in a car has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. To estimate <span class="math inline">\(\lambda\)</span> we obtain the random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>.</p>
<ol start="2" type="a">
<li class="fragment">Find the Cramer–Rao lower bound for the variance of an unbiased estimator of <span class="math inline">\(\lambda\)</span>.</li>
</ol>
</section>
<section id="efficiency" class="slide level2">
<h2>Efficiency</h2>
<p>Because the variance of <span class="math inline">\(T\)</span> must be at least <span class="math inline">\(1/(nI(\theta))\)</span>, it is natural to call <span class="math inline">\(T\)</span> an efficient estimator of <span class="math inline">\(\theta\)</span> if <span class="math inline">\(V(T) = 1/[nI(\theta)]\)</span>.</p>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Definition</strong></p>
</div>
<div class="callout-content">
<p>Let <span class="math inline">\(T\)</span> be an unbiased estimator of <span class="math inline">\(\theta\)</span>. The ratio of the lower bound to the variance of <span class="math inline">\(T\)</span> is its <strong>efficiency</strong>. Then <span class="math inline">\(T\)</span> is said to be an <strong>efficient</strong> estimator if <span class="math inline">\(T\)</span> achieves the Cramer-Rao lower bound (the efficiency is 1). An efficient estimator is a minimum variance unbiased estimator (MVUE) as discussed in section 7.1.</p>
</div>
</div>
</div>
</section>
<section id="example-42-2" class="slide level2">
<h2>Example 42</h2>
<p>Assume that the number of defects in a car has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. To estimate <span class="math inline">\(\lambda\)</span> we obtain the random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>.</p>
<ol start="3" type="a">
<li class="fragment">Use the score function to find the mle of <span class="math inline">\(\lambda\)</span> and show that the mle is an efficient estimator.</li>
</ol>
</section>
<section id="limiting-properties" class="slide level2">
<h2>Limiting Properties</h2>
<div class="callout callout-important callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Theorem</strong></p>
</div>
<div class="callout-content">
<p>Given a random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span> from a distribution with pmf or pdf <span class="math inline">\(f(x; \theta)\)</span>, assume that the set of possible <span class="math inline">\(x\)</span> values does not depend on <span class="math inline">\(\theta\)</span>. Then for large <span class="math inline">\(n\)</span> the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span> has approximately a normal distribution with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(1/[nI(\theta)]\)</span>. More precisely, the limiting distribution of <span class="math inline">\(\sqrt{n}(\hat{\theta}-\theta)\)</span> is normal with mean <span class="math inline">\(0\)</span> and variance <span class="math inline">\(1/I(\theta)\)</span>.</p>
</div>
</div>
</div>
</section>
<section id="example-42-3" class="slide level2">
<h2>Example 42</h2>
<p>Assume that the number of defects in a car has a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. To estimate <span class="math inline">\(\lambda\)</span> we obtain the random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>.</p>
<ol start="4" type="a">
<li class="fragment">Is the asymptotic distribution of the mle in accord with the second theorem? Explain.</li>
</ol>
</section>
<section id="example-44" class="slide level2">
<h2>Example 44</h2>
<p>Survival times have the exponential distribution with pdf <span class="math inline">\(f(x; \lambda) = \lambda e^{- \lambda x}\)</span>, <span class="math inline">\(x \geq 0\)</span>, and <span class="math inline">\(f(x;\lambda) = 0\)</span> otherwise, where <span class="math inline">\(\lambda &gt; 0\)</span>. However, we wish to estimate the mean <span class="math inline">\(\mu = 1/\lambda\)</span> based on the random sample <span class="math inline">\(X_{1}\)</span>, <span class="math inline">\(X_{2}\)</span>, …, <span class="math inline">\(X_{n}\)</span>, so let’s re-express the pdf in the form <span class="math inline">\((1/\mu)e^{-x/\mu}\)</span>.</p>
<ol type="a">
<li class="fragment">Find the information in a single observation and the Cramer–Rao lower bound.</li>
<li class="fragment">Use the score function to find the mle of <span class="math inline">\(\mu\)</span>.</li>
<li class="fragment">Find the mean and variance of the mle.</li>
<li class="fragment">Is the mle an efficient estimator? Explain.</li>
</ol>
</section>
<section id="summary-5" class="slide level2">
<h2>Summary</h2>
<ul>
<li class="fragment">Fisher’s Information is defined as <span class="math inline">\(I(\theta) = V\left[ \frac{\partial}{\partial\theta}\ln(f(X;\theta))\right]\)</span>.</li>
<li class="fragment">It is often easier to calculate <span class="math inline">\(I(\theta) = -E\left[ \frac{\partial^{2}}{\partial\theta^{2}}\ln f(X;\theta)\right]\)</span> for Fisher’s Information.</li>
<li class="fragment">The score function is defined as <span class="math inline">\(\frac{\partial}{\partial\theta}\ln f(X_{1}, X_{2},...,X_{n};\theta) = \frac{\partial}{\partial\theta}\ln[f(X_{1};\theta)\cdot f(X_{2};\theta)\cdots f(X_{n};\theta)]\)</span>.</li>
<li class="fragment">The Fisher information <span class="math inline">\(I_{n}(theta)\)</span> for the random sample is the variance of the <strong>score function</strong>. This gives us that <span class="math inline">\(I_{n}(\theta) - nI(\theta)\)</span> for a random sample.</li>
</ul>
</section>
<section id="summary-6" class="slide level2 smaller">
<h2>Summary</h2>
<ul>
<li class="fragment">Cramer-Rao: If a random sample is collected from a distribution such that the possible value do not depend on <span class="math inline">\(\theta\)</span>, and the statistic <span class="math inline">\(T\)</span> is an unbiased estimator, then <span class="math inline">\(V(T) \geq \frac{1}{nI(\theta)} = \frac{1}{I_{n}(\theta)}\)</span>.</li>
<li class="fragment">The ratio of the lower bound to the variance of <span class="math inline">\(T\)</span> is its <strong>efficiency</strong>.</li>
<li class="fragment">Then <span class="math inline">\(T\)</span> is said to be an <strong>efficient</strong> estimator if <span class="math inline">\(T\)</span> achieves the Cramer-Rao lower bound (the efficiency is 1). An efficient estimator is a minimum variance unbiased estimator (MVUE) as discussed in section 7.1.</li>
<li class="fragment">Assume that the set of possible <span class="math inline">\(x\)</span> values does not depend on <span class="math inline">\(\theta\)</span>. Then for large <span class="math inline">\(n\)</span> the maximum likelihood estimator <span class="math inline">\(\hat{\theta}\)</span> has approximately a normal distribution with mean <span class="math inline">\(\theta\)</span> and variance <span class="math inline">\(1/[nI(\theta)]\)</span>.</li>
</ul>
</section>
<section id="practice-problems-3" class="slide level2">
<h2>Practice Problems</h2>
<ul>
<li class="fragment">Odd problems 43 - 47</li>
</ul>
</section>
<section id="supplementary-exercises" class="slide level2">
<h2>Supplementary Exercises</h2>
<ul>
<li class="fragment">49</li>
<li class="fragment">Odd problems 53 - 57</li>
</ul>
<div class="footer footer-default">

</div>
</section></section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="Chapter7Slides_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="Chapter7Slides_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="Chapter7Slides_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="Chapter7Slides_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: false,

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        let tooltip;
        if (window.bootstrap) {
          button.setAttribute("data-bs-toggle", "tooltip");
          button.setAttribute("data-bs-placement", "left");
          button.setAttribute("data-bs-title", "Copied!");
          tooltip = new bootstrap.Tooltip(button, 
            { trigger: "manual", 
              customClass: "code-copy-button-tooltip",
              offset: [0, -8]});
          tooltip.show();    
        }
        setTimeout(function() {
          if (tooltip) {
            tooltip.hide();
            button.removeAttribute("data-bs-title");
            button.removeAttribute("data-bs-toggle");
            button.removeAttribute("data-bs-placement");
          }
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>